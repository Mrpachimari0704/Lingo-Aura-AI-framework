# ==============================================================================
#  evaluate.py (Lingo-Aura on CMU-MOSEI - Inference & Evaluation)
# ==============================================================================
#
# HOW TO RUN:
# 1. ç¡®ä¿è®­ç»ƒå·²å®Œæˆï¼Œå¹¶ä¸”æ¨¡å‹æƒé‡å·²ä¿å­˜åœ¨ MODEL_PATH æŒ‡å®šçš„ç›®å½•ä¸­ã€‚
# 2. ç¡®ä¿æ‰€æœ‰ä¾èµ–é¡¹ã€æ•°æ®æ–‡ä»¶å’Œ prompt æ¨¡æ¿éƒ½ä¸è®­ç»ƒæ—¶ç›¸åŒã€‚
# 3. ç›´æ¥è¿è¡Œ: python evaluate.py

import os
os.environ['CUDA_VISIBLE_DEVICES']='1'
import re
import json
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn


# å¯¼å…¥æˆ‘ä»¬é¡¹ç›®ä¸­çš„æ ¸å¿ƒç»„ä»¶
# å‡è®¾ evaluate.py ä¸ lingo_aura_standalone.py åœ¨åŒä¸€ç›®å½•
from train_full_model_lora_r16_normalize import  MOSEIDataset
from mmsdk import mmdatasdk as md
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig


class Config:
    DATA_PATH = "./data/cmumosei/"
    COGNITIVE_LABELS_CSV = os.path.join(DATA_PATH, "cmu_mosei_with_cognitive_labels_v4.csv")
    PROMPT_TEMPLATE_PATH = "./prompts/cognitive_informed_prompt.txt"
    OUTPUT_DIR = "output/all_model_LoRA_attention_right_label_r16_normalize1"

    LLM_NAME = "./phi-2"
    VISUAL_FEATURE_DIM = 35      # CMU_MOSEI_VisualFacet42 çš„ç‰¹å¾ç»´åº¦
    ACOUSTIC_FEATURE_DIM = 74    # CMU_MOSEI_COVAREP çš„ç‰¹å¾ç»´åº¦

    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    EPOCHS = 5
    BATCH_SIZE = 16
    LEARNING_RATE = 2e-5
    # LEARNING_RATE = 1e-4
    # GRADIENT_A

# --- 1. æ¨ç†ä¸“ç”¨æ•°æ®å¤„ç† ---
# class MOSEIEvaluationDataset(MOSEIDataset):
#     pass
    

# def create_evaluation_collate_fn(tokenizer, prompt_template):
#     def collate_fn(batch):
#         # âœ¨âœ¨âœ¨ã€å…³é”®ä¿®æ”¹ã€‘: åœ¨è¿™é‡ŒåŠ¨æ€æ„å»ºæ¨ç†æ—¶éœ€è¦çš„ prompt âœ¨âœ¨âœ¨
        
#         # ä»æ¨¡æ¿ä¸­åˆ†ç¦»å‡º human éƒ¨åˆ†
#         human_template, _ = prompt_template.split("### Assistant:")
#         human_template += "### Assistant:"

#         prompts = []
#         for item in batch:
#             # å¡«å…… Human éƒ¨åˆ†çš„æ¨¡æ¿ï¼Œä½œä¸ºæ¨¡å‹çš„è¾“å…¥
#             prompts.append(
#                 human_template.format(
#                     information_stance=item['cognitive_label'].get("Information Stance", ["N/A"])[0],
#                     reasoning_mode=item['cognitive_label'].get("Reasoning Mode", ["N/A"])[0],
#                     transcription=item['text']
#                 ).strip()
#             )
        
#         if tokenizer.pad_token is None:
#             tokenizer.pad_token = tokenizer.eos_token
        
#         # åªå¯¹ prompt éƒ¨åˆ†è¿›è¡Œåˆ†è¯
#         tokenized = tokenizer(prompts, padding='longest', return_tensors="pt")

#         return {
#             'input_ids': tokenized['input_ids'],
#             'attention_mask': tokenized['attention_mask'],
#             'visual_features': pad_sequence([f['visual'] for f in batch], batch_first=True),
#             'acoustic_features': pad_sequence([f['acoustic'] for f in batch], batch_first=True),
#             # 'ground_truth_scores' çš„æ¥æºä¿æŒä¸å˜
#             'ground_truth_scores': torch.tensor([item['emotion_score'] for item in batch])
#         }
#     return collate_fn


def create_evaluation_collate_fn(tokenizer, prompt_template):
    def collate_fn(batch):
        # --- ğŸ› ï¸ å¼ºåŠ›ä¿®æ­£ï¼šFew-Shot å¼•å¯¼ ---
        
        # 1. åŸºç¡€åˆ†å‰²
        # å‡è®¾ prompt_template é‡ŒåŒ…å« ### Assistant:
        base_human_part = prompt_template.split("### Assistant:")[0]
        
        # 2. æ„å»ºä¸€ä¸ªâ€œå‡â€çš„èŒƒä¾‹ (One-Shot Example)
        # è¿™æ˜¯ä¸€ä¸ªæ•™ç§‘ä¹¦çº§åˆ«çš„ç¤ºèŒƒï¼Œå‘Šè¯‰æ¨¡å‹ä¸è¦åºŸè¯ï¼Œç›´æ¥ç»™åˆ†ã€‚
        fake_example = (
            "Information Stance: Neutral. Reasoning Mode: Descriptive. "
            "Transcription: \"The weather is okay, just a normal day.\" "
            "\n### Assistant: Based on the multimodal features, the speaker's emotion score is 0.10."
            "\n### Human: "  # æ¢è¡Œï¼Œå‡†å¤‡æ‹¼æ¥çœŸå®çš„ Prompt
        )
        
        # 3. çœŸæ­£çš„å¼ºåˆ¶å‰ç¼€
        force_prefix = "### Assistant: Based on the multimodal features, the speaker's emotion score is"

        prompts = []
        for item in batch:
            # å¡«å……çœŸå®æ•°æ®çš„å˜é‡
            real_human_text = base_human_part.format(
                information_stance=item['cognitive_label'].get("Information Stance", "N/A"),
                reasoning_mode=item['cognitive_label'].get("Reasoning Mode", "N/A"),
                transcription=item['text']
            )
            
            # æ‹¼æ¥é€»è¾‘ï¼š[å‡èŒƒä¾‹] + [çœŸé—®é¢˜] + [å¼ºåˆ¶å‰ç¼€]
            full_prompt = fake_example + real_human_text + force_prefix
            
            prompts.append(full_prompt.strip())
        
        # 4. åˆ†è¯ (æ³¨æ„ï¼šæ¨ç†æ—¶ batch_size > 1 å¿…é¡»ç”¨ left padding)
        tokenizer.padding_side = "left" 
        tokenized = tokenizer(prompts, padding='longest', return_tensors="pt")

        return {
            'input_ids': tokenized['input_ids'],
            'attention_mask': tokenized['attention_mask'],
            'visual_features': pad_sequence([f['visual'] for f in batch], batch_first=True),
            'acoustic_features': pad_sequence([f['acoustic'] for f in batch], batch_first=True),
            'ground_truth_scores': torch.tensor([item['emotion_score'] for item in batch])
        }
    return collate_fn


class LingoAuraInferenceModel(nn.Module):
    def __init__(self, config, tokenizer, base_model, visual_projector, acoustic_projector, visual_attention, acoustic_attention):
        super().__init__()
        self.config = config
        self.tokenizer = tokenizer
        
        # self.base_model åœ¨è¿™é‡Œè¢«èµ‹å€¼ã€‚å®ƒå°±æ˜¯ä»å¤–éƒ¨ä¼ å…¥çš„ã€èåˆäº†LoRAçš„æ ¸å¿ƒLLMã€‚
        self.base_model = base_model 
        
        self.visual_projector = visual_projector
        self.acoustic_projector = acoustic_projector
        self.visual_attention = visual_attention
        self.acoustic_attention = acoustic_attention
        self.hidden_size = base_model.config.hidden_size
        self.fake_visual_token = nn.Embedding(1, config.VISUAL_FEATURE_DIM)
        self.fake_acoustic_token = nn.Embedding(1, config.ACOUSTIC_FEATURE_DIM)

        llm_device = 'cuda'
        # ç§»åŠ¨åµŒå…¥å±‚åˆ°GPU
        self.fake_visual_token = self.fake_visual_token.to(llm_device)
        self.fake_acoustic_token = self.fake_acoustic_token.to(llm_device)

    def forward(self, input_ids, attention_mask, visual_features, acoustic_features):
        # 1. å¤šæ¨¡æ€æŠ•å½±ä¸æ³¨æ„åŠ›å¤„ç†ï¼ˆä¸ä¹‹å‰å®Œå…¨ä¸€è‡´ï¼‰
        projected_visual = self.visual_projector(visual_features.to(torch.bfloat16))
        projected_acoustic = self.acoustic_projector(acoustic_features.to(torch.bfloat16))
        text_embeds = self.base_model.get_input_embeddings()(input_ids).to(torch.bfloat16)
        
        query_embed = text_embeds[:, 0:1, :]
        visual_token_embeds, _ = self.visual_attention(query=query_embed, key=projected_visual, value=projected_visual)
        acoustic_token_embeds, _ = self.acoustic_attention(query=query_embed, key=projected_acoustic, value=projected_acoustic)
        
        inputs_embeds = torch.cat([text_embeds[:, :1, :], visual_token_embeds, acoustic_token_embeds, text_embeds[:, 1:, :]], dim=1)
        
        extra_tokens_mask = torch.ones((attention_mask.shape[0], 2), device=attention_mask.device)
        final_attn_mask = torch.cat([attention_mask[:, :1], extra_tokens_mask, attention_mask[:, 1:]], dim=1)

        # 2. è°ƒç”¨ç”Ÿæˆé€»è¾‘ (é€»è¾‘ä¸å˜)
        outputs = self.base_model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=final_attn_mask,
            max_new_tokens=20,
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=False,
            temperature=0.0,
            min_new_tokens=1
        )
        return outputs
# --- ä¸»è¯„ä¼°å‡½æ•° ---
def evaluate():
    print("="*60)
    print("Lingo-Aura LLM - CMU-MOSEI æ¨¡å‹è¯„ä¼°è„šæœ¬")
    print("="*60)

    config = Config()
    MODEL_PATH = config.OUTPUT_DIR 
    DEVICE = config.DEVICE

    # --- [1/4] æ¨¡å‹åŠ è½½éƒ¨åˆ†ï¼ˆå·²ä¿®æ”¹ï¼‰ ---
    print(f"\n[1/4] æ­£åœ¨ä» '{MODEL_PATH}' åŠ è½½æ¨¡å‹...")

    # a. åœ¨åŠ è½½åŸºç¡€æ¨¡å‹æ—¶ï¼Œå°±æŒ‡å®šå¥½æœ€ç»ˆçš„ torch_dtype
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    base_model = AutoModelForCausalLM.from_pretrained(
        config.LLM_NAME,
        quantization_config=quant_config,
        device_map=config.DEVICE,
        trust_remote_code=True,
        # âœ¨âœ¨âœ¨ã€å…³é”®ä¿®æ”¹ 1ã€‘âœ¨âœ¨âœ¨
        # åœ¨åŠ è½½æ—¶å°±æ˜ç¡®æŒ‡å®šè®¡ç®—å’Œæƒé‡çš„dtypeï¼Œé˜²æ­¢åç»­è½¬æ¢
        torch_dtype=torch.bfloat16, 
    )
    tokenizer = AutoTokenizer.from_pretrained(config.LLM_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left" 

    # b. å°†LoRAé€‚é…å™¨èåˆåˆ°åŸºç¡€æ¨¡å‹ä¸­
    model = PeftModel.from_pretrained(base_model, MODEL_PATH)
    model = model.merge_and_unload()
    print(" - LoRA é€‚é…å™¨å·²åŠ è½½å¹¶èåˆã€‚")

    # c. åˆ›å»ºå¹¶åŠ è½½ Projectors
    llama_hidden_size = model.config.hidden_size

    #å¼€å§‹é‡å»ºæ¶æ„ âœ¨âœ¨âœ¨ ---
    # 1. åˆ›å»º Projector æ¨¡å—çš„â€œç©ºå£³â€
    visual_projector = nn.Linear(config.VISUAL_FEATURE_DIM, llama_hidden_size)
    acoustic_projector = nn.Linear(config.ACOUSTIC_FEATURE_DIM, llama_hidden_size)

    # 2. åˆ›å»º Attention æ¨¡å—çš„â€œç©ºå£³â€
    #    è¿™é‡Œçš„å‚æ•°ï¼ˆembed_dim, num_headsï¼‰å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
    visual_attention = nn.MultiheadAttention(embed_dim=llama_hidden_size, num_heads=4, batch_first=True)
    acoustic_attention = nn.MultiheadAttention(embed_dim=llama_hidden_size, num_heads=4, batch_first=True)

    # --- âœ¨âœ¨âœ¨ åŠ è½½æƒé‡ âœ¨âœ¨âœ¨ ---

    # 3. åŠ è½½ Projector çš„æƒé‡
    visual_projector.load_state_dict(torch.load(os.path.join(MODEL_PATH, "visual_projector.pt")))
    acoustic_projector.load_state_dict(torch.load(os.path.join(MODEL_PATH, "acoustic_projector.pt")))

    # 4. åŠ è½½ Attention æ¨¡å—çš„æƒé‡
    visual_attention.load_state_dict(torch.load(os.path.join(MODEL_PATH, "visual_attention.pt")))
    acoustic_attention.load_state_dict(torch.load(os.path.join(MODEL_PATH, "acoustic_attention.pt")))

    # --- âœ¨âœ¨âœ¨ ç§»åŠ¨è®¾å¤‡å’Œç±»å‹ï¼Œå¹¶â€œæŒ‚è½½â€åˆ°ä¸»æ¨¡å‹ä¸Š âœ¨âœ¨âœ¨ ---

    llm_device = next(model.parameters()).device
    visual_projector.to(device=llm_device, dtype=torch.bfloat16)
    acoustic_projector.to(device=llm_device, dtype=torch.bfloat16)
    visual_attention.to(device=llm_device, dtype=torch.bfloat16)
    acoustic_attention.to(device=llm_device, dtype=torch.bfloat16)

        # --- âœ¨ å…³é”®ï¼šç”¨è‡ªå®šä¹‰æ¨¡å‹ç±»æ•´åˆæ‰€æœ‰ç»„ä»¶ âœ¨ ---
    model = LingoAuraInferenceModel(
        config=config,
        tokenizer=tokenizer,
        base_model=model,
        visual_projector=visual_projector,
        acoustic_projector=acoustic_projector,
        visual_attention=visual_attention,
        acoustic_attention=acoustic_attention
    )
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    print(" - å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å·²åˆå§‹åŒ–å®Œæˆã€‚")

    print(f"\n[2/4] æ­£åœ¨åŠ è½½ CMU-MOSEI æµ‹è¯•æ•°æ®é›†...")

    # âœ¨âœ¨âœ¨ 2. åŠ è½½ä¿å­˜çš„ç»Ÿè®¡é‡ âœ¨âœ¨âœ¨
    stats_path = os.path.join(config.OUTPUT_DIR, 'normalization_stats.json')
    try:
        with open(stats_path, 'r') as f:
            stats = json.load(f)
        visual_mean = torch.tensor(stats['visual_mean'])
        visual_std = torch.tensor(stats['visual_std'])
        acoustic_mean = torch.tensor(stats['acoustic_mean'])
        acoustic_std = torch.tensor(stats['acoustic_std'])
        
        visual_stats = (visual_mean, visual_std)
        acoustic_stats = (acoustic_mean, acoustic_std)
        print(" - æˆåŠŸåŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡ã€‚")
    except FileNotFoundError:
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å½’ä¸€åŒ–æ–‡ä»¶ {stats_path}ã€‚å°†ä¸è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚")
        visual_stats = None
        acoustic_stats = None


    cognitive_df = pd.read_csv(config.COGNITIVE_LABELS_CSV)
    with open(config.PROMPT_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    # âœ¨âœ¨âœ¨ 3. å°†ç»Ÿè®¡é‡ä¼ å…¥æµ‹è¯•é›† Dataset âœ¨âœ¨âœ¨
    test_dataset = MOSEIDataset(
        cognitive_df, 
        md.cmu_mosei.standard_folds.standard_test_fold, 
        prompt_template,
        visual_stats=visual_stats,
        acoustic_stats=acoustic_stats
    )
    collate_fn = create_evaluation_collate_fn(tokenizer, prompt_template)
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE * 2, collate_fn=collate_fn, num_workers=16)
    
    
    print(f"\n[3/4] æ­£åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¨ç†...")
    all_predictions = []
    all_ground_truths = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating on Test Set"):

            # ä»batchä¸­æå–å‚æ•°ï¼ˆç¡®ä¿è®¾å¤‡ä¸€è‡´ï¼‰
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            visual_features = batch['visual_features'].to(DEVICE)
            acoustic_features = batch['acoustic_features'].to(DEVICE)
            ground_truths = batch['ground_truth_scores']

            # --- âœ¨ è°ƒç”¨æ•´åˆåçš„æ¨¡å‹ç”Ÿæˆç»“æœ âœ¨ ---
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                visual_features=visual_features,
                acoustic_features=acoustic_features
            )

            # åç»­çš„tokenè§£ç ã€åˆ†æ•°æå–é€»è¾‘ä¿æŒä¸å˜
            # prompt_lengths = [len(ids) for ids in input_ids]
            # generated_tokens = [out[L:] for out, L in zip(outputs, prompt_lengths)]
            responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)

            # âœ¨âœ¨âœ¨ æ·»åŠ è¿™è¡Œæ‰“å°è¯­å¥ âœ¨âœ¨âœ¨
            print("åŸå§‹ç”Ÿæˆå†…å®¹:", responses) 
            
            # for res in responses:
            #     match = re.search(r"[-+]?\d+(?:\.\d+)?", res)
            #     if match:
            #         pred_score = float(match.group())
            #         all_predictions.append(pred_score)
            #     else:
            #         all_predictions.append(0.0)

            # all_ground_truths.extend(batch['ground_truth_scores'].cpu().numpy())

            for response in responses:
                # æ¸…ç†ç©ºç™½
                response = response.strip()
                
                # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—
                matches = re.findall(r"[-+]?\d+(?:\.\d+)?", response)
                
                if matches:
                    # âœ¨âœ¨âœ¨ æ”¹ä¸ºå–ç¬¬ä¸€ä¸ªæ•°å­— matches[0] âœ¨âœ¨âœ¨
                    # å› ä¸ºæˆ‘ä»¬çš„ Prompt ç»“å°¾æ˜¯ "score is"ï¼Œæ‰€ä»¥ç´§æ¥ç€çš„ç¬¬ä¸€ä¸ªæ•°å­—å°±æ˜¯åˆ†æ•°
                    try:
                        val = float(matches[0])
                        
                        # èŒƒå›´æ£€æŸ¥ [-3.5, 3.5] (CMU-MOSEI èŒƒå›´æ˜¯ -3 åˆ° 3)
                        if -3.5 <= val <= 3.5:
                            all_predictions.append(val)
                        else:
                            # å¦‚æœæå–å‡ºå¥‡æ€ªçš„æ•°å­—ï¼ˆæ¯”å¦‚å¹´ä»½ï¼‰ï¼Œè¯´æ˜æå–é”™äº†ï¼Œç”± 0.0 å…œåº•
                            all_predictions.append(0.0)
                    except:
                        all_predictions.append(0.0)
                else:
                    # æ²¡æ‰¾åˆ°æ•°å­—
                    all_predictions.append(0.0)
            
            all_ground_truths.extend(batch['ground_truth_scores'].cpu().numpy())
            print("all_predictions",all_predictions)
            
    print(f"\n[4/4] è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    print("\n" + "="*20 + " æœ€ç»ˆè¯„ä¼°ç»“æœ " + "="*20)
    print("æ¨¡å‹çš„æ‰€æœ‰é¢„æµ‹å€¼:", all_predictions[:50]) # æ‰“å°å‰50ä¸ªçœ‹çœ‹
    gts = np.array(all_ground_truths)
    preds = np.array(all_predictions)

    mae = np.mean(np.abs(gts - preds))
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE) â†“: {mae:.4f}")

    acc2 = accuracy_score(gts >= 0, preds >= 0)
    print(f"äºŒå…ƒå‡†ç¡®ç‡ (Acc-2) â†‘: {acc2*100:.2f}%")

    f1 = f1_score(gts >= 0, preds >= 0, average='weighted')
    print(f"åŠ æƒF1åˆ†æ•° (F1-Score) â†‘: {f1:.4f}")

    # ä¿®æ”¹å
    valid_indices = ~np.isnan(gts) & ~np.isnan(preds)
    gts_valid = gts[valid_indices]
    preds_valid = preds[valid_indices]

    # âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®å¤ï¼šæ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦ä¸º0 âœ¨âœ¨âœ¨
    if gts_valid.size > 1 and preds_valid.size > 1 and \
    np.std(gts_valid) > 0 and np.std(preds_valid) > 0:
        
        corr = np.corrcoef(gts_valid, preds_valid)[0, 1]
        print(f"çš®å°”é€Šç›¸å…³ç³»æ•° (Corr) â†‘: {corr:.4f}")
    else:
        print("æ— æ³•è®¡ç®—ç›¸å…³ç³»æ•° (åŸå› : é¢„æµ‹å€¼æˆ–çœŸå®å€¼çš„æ ‡å‡†å·®ä¸º0ï¼Œæˆ–æœ‰æ•ˆæ•°æ®ç‚¹ä¸è¶³)ã€‚")


if __name__ == "__main__":
    evaluate()



