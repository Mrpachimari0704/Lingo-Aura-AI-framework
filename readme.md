# Lingo-Aura: Cognitive-Informed Multimodal Sentiment Analysis

A multimodal sentiment analysis framework based on large language models (Mistral-7B/Phi-2) with cognitive prompting.

## üìÇ Project Overview

This project implements a novel multimodal sentiment analysis system that introduces **Cognitive Labels** as prompts. The framework features a lightweight **Double MLP + Mean Pooling** architecture, combined with **InfoNCE contrastive learning** and **hierarchical warmup** strategy, achieving significant improvements in sentiment intensity prediction on the CMU-MOSEI dataset.

## üåü Key Features

- **Cognitive Label Integration**: Novel use of cognitive labels as prompts for enhanced sentiment understanding
- **Multimodal Fusion**: Combines text, audio (COVAREP), and visual (Facet42) features
- **Efficient Architecture**: Lightweight Double MLP with Mean Pooling design
- **Advanced Training**: InfoNCE contrastive learning with hierarchical warmup strategy
- **LoRA Fine-tuning**: Parameter-efficient adaptation of large language models

## üóÇÔ∏è File Structure

### Core Scripts

#### Training
- `train_full_model_lora_r16_normalize_Mistral7b_changeloss_doublemlp_meanpooling_contraloss0.25_Projectorwarm_dropout_singlecard.py`
  - **Recommended training script** with all optimizations
  - Features: Double MLP, Mean Pooling, Dropout(0.2), Contrastive Loss(0.25), Hierarchical Warmup
  - Single GPU training

#### Inference
- `inference_all_attention_rightlabel_normalize_mistral7bmeanpooling_dropout_warm0.25.py`
  - Inference script with Few-Shot guidance and Prefix-Forcing
  - Generates Accuracy and Correlation metrics

#### Data Processing
- `generate_cognitive_labels.py`
  - Generates cognitive labels using DeepSeek API

### Ablation Study Scripts

For reproducing experimental comparisons:

- `train_ablation_text_only.py` - Text-only baseline (no cognitive labels)
- `train_full_model_lora_r16_normalize_Mistral7b.py` - Without contrastive loss
- `train_full_model_lora_r16_normalize_Mistral7b_changeloss_doublemlp_meanpooling_contraloss1.0_Projectorwarm_singlecard.py` - Without dropout
- `..._ddp_...py` - Multi-GPU distributed training versions
- `..._nolora.py` - Full fine-tuning baseline (without LoRA)
- `..._noacoustic.py` / `..._novision.py` - Single-modality ablations
- `..._contrastloss1.0...py` - Contrastive loss weight 1.0 (vs. 0.25)

### Outputs

- `*.out` / `*.log` - Training logs
- `output/` - Model checkpoints, adapters, and normalization statistics

## üöÄ Quick Start

### 1. Environment Setup

```bash
pip install torch transformers peft pandas numpy tqdm mmsdk scikit-learn
```

**Requirements:**
- Python 3.12+
- PyTorch with CUDA support
- Transformers, PEFT libraries

### 2. Data Preparation

Ensure the following files are in `data/cmumosei/`:

- `CMU_MOSEI_VisualFacet42.csd`
- `CMU_MOSEI_COVAREP.csd`
- `CMU_MOSEI_TimestampedWords.csd`
- `CMU_MOSEI_Labels.csd`
- `cmu_mosei_with_cognitive_labels_v4.csv` (generated by `generate_cognitive_labels.py`)

### 3. Training

Single GPU training with recommended configuration:

```bash
nohup python train_full_model_lora_r16_normalize_Mistral7b_changeloss_doublemlp_meanpooling_contraloss0.25_Projectorwarm_dropout_singlecard.py > train.log 2>&1 &
```

### 4. Evaluation

Run inference with trained model:

```bash
python inference_all_attention_rightlabel_normalize_mistral7bmeanpooling_dropout_warm0.25.py
```

## üìä Performance

**Final Model: Mistral-7B + Double MLP + Contrastive Loss(0.25) + Dropout(0.2)**

| Metric | Score | Notes |
|--------|-------|-------|
| **Accuracy (Acc-2)** | ~79.9% | On par with text-only baseline, robust to noise |
| **Correlation (r)** | ~0.15 | **135% improvement** over text-only, strong intensity perception |

## üî¨ Model Architecture

### Key Components

1. **Base Model**: Mistral-7B with LoRA (r=16)
2. **Multimodal Projectors**: 
   - Text ‚Üí Hidden dimension
   - Audio (COVAREP) ‚Üí Hidden dimension
   - Visual (Facet42) ‚Üí Hidden dimension
3. **Fusion Layer**: Double MLP with Mean Pooling
4. **Regularization**: Dropout (0.2)
5. **Loss Function**: Combined MSE + InfoNCE Contrastive Loss (0.25 weight)

### Training Strategy

- **Hierarchical Warmup**: Gradual unfreezing of model components
- **Normalization**: Feature normalization for stable training
- **Contrastive Learning**: InfoNCE loss for better multimodal alignment

## üí° Key Innovations

1. **Cognitive Labels as Prompts**: Enhances model's understanding of sentiment nuances
2. **Double MLP Architecture**: Efficient feature transformation
3. **Mean Pooling**: Better representation aggregation
4. **Balanced Contrastive Loss**: 0.25 weight prevents overfitting while improving alignment
5. **Hierarchical Warmup**: Stable training progression

## üìÑ License

This project is licensed under the MIT License.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìß Contact

For questions or suggestions, please open an issue or contact [your-email@example.com].
