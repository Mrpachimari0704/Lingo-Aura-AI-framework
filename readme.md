Lingo-Aura: Cognitive-Informed Multimodal Sentiment Analysis
Lingo-Aura: Cognitive-Informed Multimodal Sentiment Analysis System
ðŸ“‚ Project Overview
This project implements a multimodal sentiment analysis framework based on large language models (Mistral-7B/Phi-2). The core innovation lies in introducing Cognitive Labels as prompts and designing a lightweight architecture of Double MLP + Mean Pooling, combined with InfoNCE contrastive learning and hierarchical warmup strategy, achieving significant improvement in sentiment intensity prediction (Correlation) on the CMU-MOSEI dataset.
ðŸ“¦ Key Deliverables
Before viewing the code, it is recommended to read the following documents first to understand the core technical approach and experimental conclusions:

æŠ€æœ¯æŠ¥å‘Š.pdf / æŠ€æœ¯æŠ¥å‘Š.docx
ðŸ“„ [Most Important] Complete project technical report. Includes model architecture diagrams, SOTA comparison, ablation experiment analysis, and final conclusions.
æŠ€æœ¯å›¾ç‰‡.pptx
ðŸ“Š Editable source files for all architecture diagrams in the report.


ðŸ—‚ï¸ File Structure
The folder contains scripts and logs from multiple experimental iterations. The following is a classification of key files:
1. Main Scripts
This is the final version with the best validated performance, recommended for use:

Training:

train_full_model_lora_r16_normalize_Mistral7b_changeloss_doublemlp_meanpooling_contraloss0.25_Projectorwarm_dropout_singlecard.py
Description: This is the final winning solution. A single-card training script integrating Double MLP, Mean Pooling, Dropout(0.2), contrastive loss (0.25 weight), and hierarchical warmup strategy.


Inference:

inference_all_attention_rightlabel_normalize_mistral7bmeanpooling_dropout_warm0.25.py (need to confirm the specific inference script filename used, usually matching the above training script)
Description: Includes Few-Shot guidance and Prefix-Forcing strategy for generating final Acc and Corr metrics.


Data Processing:

generate_cognitive_labels.py: Script for calling DeepSeek API to generate cognitive labels.



2. Ablation & History
To reproduce the comparative experiments in the report, the following variant scripts are retained:

train_ablation_text_only.py: Text-only ablation experiment script (no cognitive)
train_full_model_lora_r16_normalize_Mistral7b.py: Ablation experiment script without contrastive loss, etc.
train_full_model_lora_r16_normalize_Mistral7b_changeloss_doublemlp_meanpooling_contraloss1.0_Projectorwarm_singlecard.py: Ablation experiment script without dropout
..._ddp_...py: Multi-GPU distributed training version (for acceleration, but more complex configuration).
..._nolora.py: Full fine-tuning or frozen baseline without LoRA (for comparison).
..._noacoustic.py / ..._novision.py: Single-modality ablation experiment scripts.
..._contrastloss1.0...py: Experimental version with contrastive loss weight of 1.0 (less effective than 0.25).
......:

3. Logs & Outputs

*.out / *.log: Console log records of the training process.
output/: Directory for saving model weights (Checkpoint), adapters (Adapter), and normalization statistics.


ðŸš€ Quick Start
1. Environment Setup
Ensure Python 3.12+ is installed along with the following core libraries:
bashpip install torch transformers peft pandas numpy tqdm mmsdk scikit-learn
2. Data Preparation
Please ensure the data/cmumosei/ directory contains the following files:

CMU_MOSEI_VisualFacet42.csd
CMU_MOSEI_COVAREP.csd
CMU_MOSEI_TimestampedWords.csd
CMU_MOSEI_Labels.csd
cmu_mosei_with_cognitive_labels_v4.csv (generated by generate_cognitive_labels.py)

3. Training
Train with the final recommended configuration (single-card mode):
bashnohup python train_full_model_lora_r16_normalize_Mistral7b_changeloss_doublemlp_meanpooling_contraloss0.25_Projectorwarm_dropout_singlecard.py > train.log 2>&1 &
4. Evaluation
Load trained weights for testing:
bashpython inference_all_attention_rightlabel_normalize_mistral7bmeanpooling_dropout_warm0.25.py

ðŸ“Š Experimental Conclusions Overview
Based on the final model (Mistral-7B + Double MLP + Contrastive 0.25 + Dropout):

Accuracy (Acc-2): ~79.9% (on par with text-only baseline, successful noise resistance)
Correlation (r): ~0.15 (compared to text-only, 135% improvement, acquired sentiment intensity perception capability)

For detailed analysis, please refer to æŠ€æœ¯æŠ¥å‘Š.pdf.
