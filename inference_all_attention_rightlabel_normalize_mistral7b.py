# ==============================================================================
#  evaluate.py (Lingo-Aura on CMU-MOSEI - Inference & Evaluation)
# ==============================================================================
#
# HOW TO RUN:
# 1. ç¡®ä¿è®­ç»ƒå·²å®Œæˆï¼Œå¹¶ä¸”æ¨¡å‹æƒé‡å·²ä¿å­˜åœ¨ MODEL_PATH æŒ‡å®šçš„ç›®å½•ä¸­ã€‚
# 2. ç¡®ä¿æ‰€æœ‰ä¾èµ–é¡¹ã€æ•°æ®æ–‡ä»¶å’Œ prompt æ¨¡æ¿éƒ½ä¸è®­ç»ƒæ—¶ç›¸åŒã€‚
# 3. ç›´æ¥è¿è¡Œ: python evaluate.py

import os
os.environ['CUDA_VISIBLE_DEVICES']='1'
import re
import json
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn


# å¯¼å…¥æˆ‘ä»¬é¡¹ç›®ä¸­çš„æ ¸å¿ƒç»„ä»¶
# å‡è®¾ evaluate.py ä¸ lingo_aura_standalone.py åœ¨åŒä¸€ç›®å½•
from train_full_model_lora_r16_normalize import  MOSEIDataset
from mmsdk import mmdatasdk as md
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig


class Config:
    DATA_PATH = "./data/cmumosei/"
    COGNITIVE_LABELS_CSV = os.path.join(DATA_PATH, "cmu_mosei_with_cognitive_labels_v4.csv")
    PROMPT_TEMPLATE_PATH = "./prompts/cognitive_informed_prompt.txt"
    OUTPUT_DIR = "output/all_model_LoRA_attention_right_label_r16_normalize_mistral7b_changeloss_ddp_changelr"

    LLM_NAME = "./Mistral-7B-Instruct-v0.2"
    VISUAL_FEATURE_DIM = 35      # CMU_MOSEI_VisualFacet42 çš„ç‰¹å¾ç»´åº¦
    ACOUSTIC_FEATURE_DIM = 74    # CMU_MOSEI_COVAREP çš„ç‰¹å¾ç»´åº¦

    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    EPOCHS = 5
    BATCH_SIZE = 16
    LEARNING_RATE = 1e-5
    # LEARNING_RATE = 1e-4
    # GRADIENT_A

# --- 1. æ¨ç†ä¸“ç”¨æ•°æ®å¤„ç† ---
# class MOSEIEvaluationDataset(MOSEIDataset):
#     pass
    

# def create_evaluation_collate_fn(tokenizer, prompt_template):
#     def collate_fn(batch):
#         # âœ¨âœ¨âœ¨ã€å…³é”®ä¿®æ”¹ã€‘: åœ¨è¿™é‡ŒåŠ¨æ€æ„å»ºæ¨ç†æ—¶éœ€è¦çš„ prompt âœ¨âœ¨âœ¨
        
#         # ä»æ¨¡æ¿ä¸­åˆ†ç¦»å‡º human éƒ¨åˆ†
#         human_template, _ = prompt_template.split("### Assistant:")
#         human_template += "### Assistant:"

#         prompts = []
#         for item in batch:
#             # å¡«å…… Human éƒ¨åˆ†çš„æ¨¡æ¿ï¼Œä½œä¸ºæ¨¡å‹çš„è¾“å…¥
#             prompts.append(
#                 human_template.format(
#                     information_stance=item['cognitive_label'].get("Information Stance", ["N/A"])[0],
#                     reasoning_mode=item['cognitive_label'].get("Reasoning Mode", ["N/A"])[0],
#                     transcription=item['text']
#                 ).strip()
#             )
        
#         if tokenizer.pad_token is None:
#             tokenizer.pad_token = tokenizer.eos_token
        
#         # åªå¯¹ prompt éƒ¨åˆ†è¿›è¡Œåˆ†è¯
#         tokenized = tokenizer(prompts, padding='longest', return_tensors="pt")

#         return {
#             'input_ids': tokenized['input_ids'],
#             'attention_mask': tokenized['attention_mask'],
#             'visual_features': pad_sequence([f['visual'] for f in batch], batch_first=True),
#             'acoustic_features': pad_sequence([f['acoustic'] for f in batch], batch_first=True),
#             # 'ground_truth_scores' çš„æ¥æºä¿æŒä¸å˜
#             'ground_truth_scores': torch.tensor([item['emotion_score'] for item in batch])
#         }
#     return collate_fn


#########ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼Œç”Ÿæˆå›ç­”çš„å‰åŠéƒ¨åˆ†
# def create_evaluation_collate_fn(tokenizer, prompt_template):
#     def collate_fn(batch):
#         # --- ğŸ› ï¸ å¼ºåˆ¶ä¿®æ­£éƒ¨åˆ†å¼€å§‹ ---
#         # ä¸å†å»åˆ‡åˆ†æ–‡ä»¶é‡Œçš„æ¨¡æ¿äº†ï¼Œå› ä¸ºæ–‡ä»¶æ ¼å¼å®¹æ˜“å‡ºé”™ã€‚
#         # æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºä¸€ä¸ªâ€œæœ€å¼ºå¼•å¯¼â€çš„ Promptã€‚
        
#         # 1. æå– Human éƒ¨åˆ† (å³ ### Assistant: ä¹‹å‰çš„æ‰€æœ‰å†…å®¹)
#         base_human_part = prompt_template.split("### Assistant:")[0]
        
#         # 2. æ‰‹åŠ¨æ‹¼æ¥ Assistant çš„å¼•å¯¼è¯
#         # è¿™ä¸€å¥ "The speaker's emotion score is" æ˜¯å…³é”®ï¼
#         # å®ƒå‘Šè¯‰æ¨¡å‹ï¼šåˆ«åºŸè¯äº†ï¼Œç›´æ¥ç»™æˆ‘æ¥æ•°å­—ã€‚
#         force_prefix = "### Assistant: Based on the multimodal features, the speaker's emotion score is"

#         prompts = []
#         for item in batch:
#             # å¡«å……å˜é‡
#             human_text = base_human_part.format(
#                 information_stance=item['cognitive_label'].get("Information Stance", "N/A"),
#                 reasoning_mode=item['cognitive_label'].get("Reasoning Mode", "N/A"),
#                 transcription=item['text']
#             )
            
#             # æ‹¼æ¥ï¼šHumanè¾“å…¥ + å¼ºåˆ¶å¼•å¯¼
#             # æœ€ç»ˆ Prompt ç»“å°¾ä¸ä»…æ˜¯ Assistant:ï¼Œè€Œæ˜¯ Assistant: ... score is
#             full_prompt = human_text + force_prefix
#             prompts.append(full_prompt.strip())
#         # --- ğŸ› ï¸ å¼ºåˆ¶ä¿®æ­£éƒ¨åˆ†ç»“æŸ ---
        
#         tokenized = tokenizer(prompts, padding='longest', return_tensors="pt")

#         return {
#             'input_ids': tokenized['input_ids'],
#             'attention_mask': tokenized['attention_mask'],
#             'visual_features': pad_sequence([f['visual'] for f in batch], batch_first=True),
#             'acoustic_features': pad_sequence([f['acoustic'] for f in batch], batch_first=True),
#             'ground_truth_scores': torch.tensor([item['emotion_score'] for item in batch])
#         }
#     return collate_fn




def create_evaluation_collate_fn(tokenizer, prompt_template):
    def collate_fn(batch):
        # --- ğŸ› ï¸ å¼ºåŠ›ä¿®æ­£ï¼šFew-Shot å¼•å¯¼ ---
        
        # 1. åŸºç¡€åˆ†å‰²
        # å‡è®¾ prompt_template é‡ŒåŒ…å« ### Assistant:
        base_human_part = prompt_template.split("### Assistant:")[0]
        
        # 2. æ„å»ºä¸€ä¸ªâ€œå‡â€çš„èŒƒä¾‹ (One-Shot Example)
        # è¿™æ˜¯ä¸€ä¸ªæ•™ç§‘ä¹¦çº§åˆ«çš„ç¤ºèŒƒï¼Œå‘Šè¯‰æ¨¡å‹ä¸è¦åºŸè¯ï¼Œç›´æ¥ç»™åˆ†ã€‚
        fake_example = (
            "Information Stance: Neutral. Reasoning Mode: Descriptive. "
            "Transcription: \"The weather is okay, just a normal day.\" "
            "\n### Assistant: Based on the multimodal features, the speaker's emotion score is 0.10."
            "\n### Human: "  # æ¢è¡Œï¼Œå‡†å¤‡æ‹¼æ¥çœŸå®çš„ Prompt
        )
        
        # 3. çœŸæ­£çš„å¼ºåˆ¶å‰ç¼€
        force_prefix = "### Assistant: Based on the multimodal features, the speaker's emotion score is"

        prompts = []
        for item in batch:
            # å¡«å……çœŸå®æ•°æ®çš„å˜é‡
            real_human_text = base_human_part.format(
                information_stance=item['cognitive_label'].get("Information Stance", "N/A"),
                reasoning_mode=item['cognitive_label'].get("Reasoning Mode", "N/A"),
                transcription=item['text']
            )
            
            # æ‹¼æ¥é€»è¾‘ï¼š[å‡èŒƒä¾‹] + [çœŸé—®é¢˜] + [å¼ºåˆ¶å‰ç¼€]
            full_prompt = fake_example + real_human_text + force_prefix
            
            prompts.append(full_prompt.strip())
        
        # 4. åˆ†è¯ (æ³¨æ„ï¼šæ¨ç†æ—¶ batch_size > 1 å¿…é¡»ç”¨ left padding)
        tokenizer.padding_side = "left" 
        tokenized = tokenizer(prompts, padding='longest', return_tensors="pt")

        return {
            'input_ids': tokenized['input_ids'],
            'attention_mask': tokenized['attention_mask'],
            'visual_features': pad_sequence([f['visual'] for f in batch], batch_first=True),
            'acoustic_features': pad_sequence([f['acoustic'] for f in batch], batch_first=True),
            'ground_truth_scores': torch.tensor([item['emotion_score'] for item in batch])
        }
    return collate_fn


class LingoAuraInferenceModel(nn.Module):
    def __init__(self, config, tokenizer, base_model, visual_projector, acoustic_projector, visual_attention, acoustic_attention):
        super().__init__()
        self.config = config
        self.tokenizer = tokenizer
        
        # self.base_model åœ¨è¿™é‡Œè¢«èµ‹å€¼ã€‚å®ƒå°±æ˜¯ä»å¤–éƒ¨ä¼ å…¥çš„ã€èåˆäº†LoRAçš„æ ¸å¿ƒLLMã€‚
        self.base_model = base_model 
        
        self.visual_projector = visual_projector
        self.acoustic_projector = acoustic_projector
        self.visual_attention = visual_attention
        self.acoustic_attention = acoustic_attention
        self.hidden_size = base_model.config.hidden_size
        self.fake_visual_token = nn.Embedding(1, config.VISUAL_FEATURE_DIM)
        self.fake_acoustic_token = nn.Embedding(1, config.ACOUSTIC_FEATURE_DIM)

        llm_device = 'cuda'
        # ç§»åŠ¨åµŒå…¥å±‚åˆ°GPU
        self.fake_visual_token = self.fake_visual_token.to(llm_device)
        self.fake_acoustic_token = self.fake_acoustic_token.to(llm_device)

    def forward(self, input_ids, attention_mask, visual_features, acoustic_features):
        # 1. å¤šæ¨¡æ€æŠ•å½±ä¸æ³¨æ„åŠ›å¤„ç†ï¼ˆä¸ä¹‹å‰å®Œå…¨ä¸€è‡´ï¼‰
        projected_visual = self.visual_projector(visual_features.to(torch.bfloat16))
        projected_acoustic = self.acoustic_projector(acoustic_features.to(torch.bfloat16))
        text_embeds = self.base_model.get_input_embeddings()(input_ids).to(torch.bfloat16)
        
        query_embed = text_embeds[:, 0:1, :]
        visual_token_embeds, _ = self.visual_attention(query=query_embed, key=projected_visual, value=projected_visual)
        acoustic_token_embeds, _ = self.acoustic_attention(query=query_embed, key=projected_acoustic, value=projected_acoustic)
        
        inputs_embeds = torch.cat([text_embeds[:, :1, :], visual_token_embeds, acoustic_token_embeds, text_embeds[:, 1:, :]], dim=1)

        extra_tokens_mask = torch.ones((attention_mask.shape[0], 2), device=attention_mask.device)
        final_attn_mask = torch.cat([attention_mask[:, :1], extra_tokens_mask, attention_mask[:, 1:]], dim=1)

        # inputs_embeds=text_embeds
        # final_attn_mask = attention_mask  # ä¸æ‰©å±•æ©ç 

        # 2. è°ƒç”¨ç”Ÿæˆé€»è¾‘ (é€»è¾‘ä¸å˜)
        outputs = self.base_model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=final_attn_mask,
            max_new_tokens=20,
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=False,  # å¼€å¯é‡‡æ ·ï¼ˆå…³é”®ï¼ï¼‰
            temperature=0.9,  # å¢åŠ éšæœºæ€§ï¼Œé¿å…ç”Ÿæˆç©ºå†…å®¹
            top_p=0.95,        # æ ¸é‡‡æ ·ï¼Œè¿›ä¸€æ­¥ç¨³å®šç”Ÿæˆ
            top_k=50,  
            min_new_tokens=1
        )

        return outputs
# --- ä¸»è¯„ä¼°å‡½æ•° ---
def evaluate():
    print("="*60)
    print("Lingo-Aura LLM - CMU-MOSEI æ¨¡å‹è¯„ä¼°è„šæœ¬")
    print("="*60)

    config = Config()
    MODEL_PATH = config.OUTPUT_DIR 
    DEVICE = config.DEVICE

    # --- [1/4] æ¨¡å‹åŠ è½½éƒ¨åˆ†ï¼ˆå·²ä¿®æ”¹ï¼‰ ---
    print(f"\n[1/4] æ­£åœ¨ä» '{MODEL_PATH}' åŠ è½½æ¨¡å‹...")

    # a. åœ¨åŠ è½½åŸºç¡€æ¨¡å‹æ—¶ï¼Œå°±æŒ‡å®šå¥½æœ€ç»ˆçš„ torch_dtype
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    base_model = AutoModelForCausalLM.from_pretrained(
        config.LLM_NAME,
        quantization_config=quant_config,
        device_map=config.DEVICE,
        trust_remote_code=True,
        # âœ¨âœ¨âœ¨ã€å…³é”®ä¿®æ”¹ 1ã€‘âœ¨âœ¨âœ¨
        # åœ¨åŠ è½½æ—¶å°±æ˜ç¡®æŒ‡å®šè®¡ç®—å’Œæƒé‡çš„dtypeï¼Œé˜²æ­¢åç»­è½¬æ¢
        torch_dtype=torch.bfloat16, 
    )
    tokenizer = AutoTokenizer.from_pretrained(config.LLM_NAME, trust_remote_code=True)
    tokenizer.padding_side = "left" 
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # b. å°†LoRAé€‚é…å™¨èåˆåˆ°åŸºç¡€æ¨¡å‹ä¸­
    model = PeftModel.from_pretrained(base_model, MODEL_PATH)
    model = model.merge_and_unload()
    print(" - LoRA é€‚é…å™¨å·²åŠ è½½å¹¶èåˆã€‚")

    # c. åˆ›å»ºå¹¶åŠ è½½ Projectors
    llama_hidden_size = model.config.hidden_size

    #å¼€å§‹é‡å»ºæ¶æ„ âœ¨âœ¨âœ¨ ---
    # 1. åˆ›å»º Projector æ¨¡å—çš„â€œç©ºå£³â€
    visual_projector = nn.Linear(config.VISUAL_FEATURE_DIM, llama_hidden_size)
    acoustic_projector = nn.Linear(config.ACOUSTIC_FEATURE_DIM, llama_hidden_size)

    # 2. åˆ›å»º Attention æ¨¡å—çš„â€œç©ºå£³â€
    #    è¿™é‡Œçš„å‚æ•°ï¼ˆembed_dim, num_headsï¼‰å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
    visual_attention = nn.MultiheadAttention(embed_dim=llama_hidden_size, num_heads=4, batch_first=True)
    acoustic_attention = nn.MultiheadAttention(embed_dim=llama_hidden_size, num_heads=4, batch_first=True)

    # --- âœ¨âœ¨âœ¨ åŠ è½½æƒé‡ âœ¨âœ¨âœ¨ ---

    # 3. åŠ è½½ Projector çš„æƒé‡
    visual_projector.load_state_dict(torch.load(os.path.join(MODEL_PATH, "visual_projector.pt")))
    acoustic_projector.load_state_dict(torch.load(os.path.join(MODEL_PATH, "acoustic_projector.pt")))

    # 4. åŠ è½½ Attention æ¨¡å—çš„æƒé‡
    visual_attention.load_state_dict(torch.load(os.path.join(MODEL_PATH, "visual_attention.pt")))
    acoustic_attention.load_state_dict(torch.load(os.path.join(MODEL_PATH, "acoustic_attention.pt")))

    # --- âœ¨âœ¨âœ¨ ç§»åŠ¨è®¾å¤‡å’Œç±»å‹ï¼Œå¹¶â€œæŒ‚è½½â€åˆ°ä¸»æ¨¡å‹ä¸Š âœ¨âœ¨âœ¨ ---

    llm_device = next(model.parameters()).device
    visual_projector.to(device=llm_device, dtype=torch.bfloat16)
    acoustic_projector.to(device=llm_device, dtype=torch.bfloat16)
    visual_attention.to(device=llm_device, dtype=torch.bfloat16)
    acoustic_attention.to(device=llm_device, dtype=torch.bfloat16)

        # --- âœ¨ å…³é”®ï¼šç”¨è‡ªå®šä¹‰æ¨¡å‹ç±»æ•´åˆæ‰€æœ‰ç»„ä»¶ âœ¨ ---
    model = LingoAuraInferenceModel(
        config=config,
        tokenizer=tokenizer,
        base_model=model,
        visual_projector=visual_projector,
        acoustic_projector=acoustic_projector,
        visual_attention=visual_attention,
        acoustic_attention=acoustic_attention
    )
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    print(" - å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å·²åˆå§‹åŒ–å®Œæˆã€‚")

    print(f"\n[2/4] æ­£åœ¨åŠ è½½ CMU-MOSEI æµ‹è¯•æ•°æ®é›†...")

    # âœ¨âœ¨âœ¨ 2. åŠ è½½ä¿å­˜çš„ç»Ÿè®¡é‡ âœ¨âœ¨âœ¨
    stats_path = os.path.join(config.OUTPUT_DIR, 'normalization_stats.json')
    try:
        with open(stats_path, 'r') as f:
            stats = json.load(f)
        visual_mean = torch.tensor(stats['visual_mean'])
        visual_std = torch.tensor(stats['visual_std'])
        acoustic_mean = torch.tensor(stats['acoustic_mean'])
        acoustic_std = torch.tensor(stats['acoustic_std'])
        
        visual_stats = (visual_mean, visual_std)
        acoustic_stats = (acoustic_mean, acoustic_std)
        print(" - æˆåŠŸåŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡ã€‚")
    except FileNotFoundError:
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å½’ä¸€åŒ–æ–‡ä»¶ {stats_path}ã€‚å°†ä¸è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚")
        visual_stats = None
        acoustic_stats = None


    cognitive_df = pd.read_csv(config.COGNITIVE_LABELS_CSV)
    with open(config.PROMPT_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    # âœ¨âœ¨âœ¨ 3. å°†ç»Ÿè®¡é‡ä¼ å…¥æµ‹è¯•é›† Dataset âœ¨âœ¨âœ¨
    test_dataset = MOSEIDataset(
        cognitive_df, 
        md.cmu_mosei.standard_folds.standard_test_fold, 
        prompt_template,
        visual_stats=visual_stats,
        acoustic_stats=acoustic_stats
    )
    collate_fn = create_evaluation_collate_fn(tokenizer, prompt_template)
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE *2, collate_fn=collate_fn, num_workers=16)
    
    
    print(f"\n[3/4] æ­£åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¨ç†...")
    all_predictions = []
    all_ground_truths = []

    # with torch.no_grad():
    #     for batch in tqdm(test_loader):
    #         input_ids = batch['input_ids'].to(config.DEVICE)
    #         attention_mask = batch['attention_mask'].to(config.DEVICE)
    #         visual_features = batch['visual_features'].to(config.DEVICE)
    #         acoustic_features = batch['acoustic_features'].to(config.DEVICE)
            
    #         outputs = model(input_ids, attention_mask, visual_features, acoustic_features)
            
    #         # âœ¨âœ¨âœ¨ ä¿®å¤ 3: æ­£ç¡®çš„æˆªå–é€»è¾‘ âœ¨âœ¨âœ¨
    #         # æ–¹æ³•ï¼šå°†ç”Ÿæˆçš„ ID è½¬ä¸ºæ–‡æœ¬ï¼Œç„¶ååŒ¹é… prompt
    #         full_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    #         prompt_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
            
    #         print(f"è°ƒè¯• - åŸå§‹è¾“å‡ºæ ·ä¾‹: {full_texts}") # æ‰“å°çœ‹çœ‹

    #         # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜åœ¨ç”ŸæˆåºŸè¯
    #         if len(all_predictions) == 0:
    #             print(f"\nè°ƒè¯• - å½“å‰ Prompt ç»“å°¾: ...{tokenizer.decode(input_ids[0])[-50:]}")
    #             print(f"è°ƒè¯• - æ¨¡å‹ç”Ÿæˆå†…å®¹: {full_texts[0]}")

    #         # for full_text in full_texts:
    #         #     # ç®€åŒ–æå–é€»è¾‘ï¼šç›´æ¥åœ¨æ•´æ®µæ–‡æœ¬é‡Œæ‰¾æœ€åä¸€ä¸ªå‡ºç°çš„æ•°å­—
    #         #     # å› ä¸º Prompt å·²ç»å¼•å¯¼åˆ°äº† "score is"ï¼Œåé¢ç´§æ¥ç€çš„ä¸€å®šæ˜¯æ•°å­—
                
    #         #     # æŸ¥æ‰¾æ‰€æœ‰æµ®ç‚¹æ•°æ ¼å¼
    #         #     matches = re.findall(r"[-+]?\d+(?:\.\d+)?", full_text)
                
    #         #     if matches:
    #         #         # å–æœ€åä¸€ä¸ªæ•°å­—ï¼ˆé€šå¸¸æ˜¯æœ€é åçš„é‚£ä¸ªç”Ÿæˆçš„é¢„æµ‹å€¼ï¼‰
    #         #         try:
    #         #             val = float(matches[-1])
    #         #             # é™åˆ¶ä¸€ä¸‹èŒƒå›´ï¼Œé˜²æ­¢æå–åˆ°å¥‡æ€ªçš„å¹´ä»½æ•°æ®
    #         #             if -3.5 <= val <= 3.5:
    #         #                 all_predictions.append(val)
    #         #             else:
    #         #                 # å¦‚æœæ•°å­—å¤ªç¦»è°±ï¼ˆæ¯”å¦‚ 2024ï¼‰ï¼Œå¯èƒ½æ˜¯æå–é”™äº†ï¼Œå­˜0
    #         #                 all_predictions.append(0.0) 
    #         #         except:
    #         #             all_predictions.append(0.0)
    #         #     else:
    #         #         all_predictions.append(0.0)

    #         for full_text, prompt_text in zip(full_texts, prompt_texts):
    #             response = full_text[len(prompt_text):].strip()
                
    #             # æ‰“å°ä¸€ä¸‹ response çœ‹çœ‹ç°åœ¨å˜ä¹–äº†æ²¡
    #             # print(f"Response: {response}") 

    #             # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—
    #             matches = re.findall(r"[-+]?\d+(?:\.\d+)?", response)
                
    #             if matches:
    #                 # âœ¨âœ¨ æ”¹ä¸ºå–æœ€åä¸€ä¸ªæ•°å­— matches[-1] âœ¨âœ¨
    #                 # å› ä¸ºå³ä½¿æ¨¡å‹åºŸè¯äº†ä¸€å †ï¼Œé€šå¸¸ç»“è®ºåœ¨æœ€å
    #                 # ä½†åœ¨ Few-Shot ä¸‹ï¼Œé€šå¸¸åªæœ‰ä¸€ä¸ªæ•°å­—ï¼Œå– -1 ä¹Ÿæ²¡é—®é¢˜
    #                 try:
    #                     val = float(matches[-1])
    #                     # ç®€å•çš„è¿‡æ»¤ï¼Œé˜²æ­¢æå–åˆ°å¹´ä»½
    #                     if -3.5 <= val <= 3.5:
    #                         all_predictions.append(val)
    #                     else:
    #                         all_predictions.append(0.0)
    #                 except:
    #                     all_predictions.append(0.0)
    #             else:
    #                 all_predictions.append(0.0)
            
    #         all_ground_truths.extend(batch['ground_truth_scores'].cpu().numpy())

    with torch.no_grad():
        for batch in tqdm(test_loader):
            input_ids = batch['input_ids'].to(config.DEVICE)
            attention_mask = batch['attention_mask'].to(config.DEVICE)
            visual_features = batch['visual_features'].to(config.DEVICE)
            acoustic_features = batch['acoustic_features'].to(config.DEVICE)
            
            # 1. æ¨¡å‹ç”Ÿæˆ (åŒ…å« Prompt + Generated)
            outputs = model(input_ids, attention_mask, visual_features, acoustic_features)
            
            # âœ¨âœ¨âœ¨ ä¿®å¤æ ¸å¿ƒï¼šä½¿ç”¨ Token åˆ‡ç‰‡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²åˆ‡ç‰‡ âœ¨âœ¨âœ¨
            # input_ids.shape[1] æ˜¯ Prompt çš„é•¿åº¦
            # æˆ‘ä»¬åªä¿ç•™ Prompt ä¹‹åç”Ÿæˆçš„ Token
            # generated_tokens = outputs[:, input_ids.shape[1]:]
            
            # è§£ç ç”Ÿæˆçš„è¿™éƒ¨åˆ† (æ­¤æ—¶ response åªæœ‰ "0.35, leaning towards...")
            responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            
            # è°ƒè¯•æ‰“å°ï¼ˆåªæ‰“å°ç¬¬ä¸€ä¸ªï¼‰
            if len(all_predictions) == 0:
                print(f"Debug - çº¯ç”Ÿæˆå†…å®¹: '{responses[0]}'")

            for response in responses:
                # æ¸…ç†ç©ºç™½
                response = response.strip()
                
                # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—
                matches = re.findall(r"[-+]?\d+(?:\.\d+)?", response)
                
                if matches:
                    # âœ¨âœ¨âœ¨ æ”¹ä¸ºå–ç¬¬ä¸€ä¸ªæ•°å­— matches[0] âœ¨âœ¨âœ¨
                    # å› ä¸ºæˆ‘ä»¬çš„ Prompt ç»“å°¾æ˜¯ "score is"ï¼Œæ‰€ä»¥ç´§æ¥ç€çš„ç¬¬ä¸€ä¸ªæ•°å­—å°±æ˜¯åˆ†æ•°
                    try:
                        val = float(matches[0])
                        
                        # èŒƒå›´æ£€æŸ¥ [-3.5, 3.5] (CMU-MOSEI èŒƒå›´æ˜¯ -3 åˆ° 3)
                        if -3.5 <= val <= 3.5:
                            all_predictions.append(val)
                        else:
                            # å¦‚æœæå–å‡ºå¥‡æ€ªçš„æ•°å­—ï¼ˆæ¯”å¦‚å¹´ä»½ï¼‰ï¼Œè¯´æ˜æå–é”™äº†ï¼Œç”± 0.0 å…œåº•
                            all_predictions.append(0.0)
                    except:
                        all_predictions.append(0.0)
                else:
                    # æ²¡æ‰¾åˆ°æ•°å­—
                    all_predictions.append(0.0)
            
            all_ground_truths.extend(batch['ground_truth_scores'].cpu().numpy())
            print("all_predictions",all_predictions)

    # è®¡ç®—æŒ‡æ ‡...
    preds, gts = np.array(all_predictions), np.array(all_ground_truths)
    mae = np.mean(np.abs(gts - preds))
    acc2 = accuracy_score(gts >= 0, preds >= 0)
    f1 = f1_score(gts >= 0, preds >= 0, average='weighted')
    print(f"MAE: {mae:.4f}, Acc-2: {acc2:.4f}, F1: {f1:.4f}")
    if len(preds) > 1 and np.std(preds) > 0:
        print(f"Corr: {np.corrcoef(gts, preds)[0, 1]:.4f}")

if __name__ == "__main__":
    evaluate()